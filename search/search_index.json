{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"LakeOps","text":"<p>A modern data lake operations toolkit working with multiple table formats (Delta, Iceberg, Parquet) and engines  (Spark, Polars) via the same APIs.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Multi-format support: Delta, Iceberg, Parquet, JSON, CSV</li> <li>Multiple engine backends: Polars, Apache Spark, Trino, Google Sheets</li> <li>Storage operations: read, write, execute SQL query</li> <li>Secrets management: local (SQLLite), Databricks</li> </ul>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install lakeops\n</code></pre>"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code>from lakeops import LakeOps\n\n# Initialize with default Polars engine\nops = LakeOps()\n\n# Read Delta table\ndf = ops.read(\"path/to/table\", format=\"delta\")\n\n# Write as Iceberg\nops.write(df, \"local.db.table\", format=\"iceberg\")\n</code></pre>"},{"location":"apis/ops.execute/","title":"ops.execute","text":"<p>LakeOps provides methods for user management including creation, read and write dataset from/to a datalake through a storage path or schema/table name.</p>"},{"location":"apis/ops.execute/#lakeops.LakeOps.execute","title":"<code>execute(sql, **kwargs)</code>","text":"<p>Execute a SQL query and return the result.</p> <p>Parameters:</p> Name Type Description Default <code>sql</code> <code>str</code> <p>SQL query to execute</p> required <code>**kwargs</code> <p>Additional arguments to pass to the underlying engine</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>pyspark.sql.DataFrame: If using SparkEngine</p> <code>Any</code> <p>polars.DataFrame: If using PolarsEngine (default) or TrinoEngine</p>"},{"location":"apis/ops.read/","title":"ops.read","text":"<p>LakeOps provides methods for user management including creation, read and write dataset from/to a datalake through a storage path or schema/table name.</p>"},{"location":"apis/ops.read/#lakeops.LakeOps.read","title":"<code>read(path, format='delta', options=None)</code>","text":"<p>Read a table from a storage path or schema/table name.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Storage path or schema/table name or Google Sheet ID to read from</p> required <code>format</code> <code>str</code> <p>Table format (default: 'delta')</p> <code>'delta'</code> <code>options</code> <code>Optional[Dict[str, Any]]</code> <p>Additional options to pass to the underlying engine</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>pyspark.sql.DataFrame: If using SparkEngine</p> <code>Any</code> <p>polars.DataFrame: If using PolarsEngine (default)</p>"},{"location":"apis/ops.write/","title":"ops.write","text":"<p>LakeOps provides methods for user management including creation, read and write dataset from/to a datalake through a storage path or schema/table name.</p>"},{"location":"apis/ops.write/#lakeops.LakeOps.write","title":"<code>write(data, path, format='delta', mode='overwrite', options=None)</code>","text":"<p>Write a table to a storage path or schema/table name.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>Data to write (DataFrame)</p> required <code>path</code> <code>str</code> <p>Storage path or schema/table name or Google Sheet ID to write to</p> required <code>format</code> <code>str</code> <p>Table format (default: 'delta')</p> <code>'delta'</code> <code>mode</code> <code>str</code> <p>Write mode - 'overwrite', 'append', etc. (default: 'overwrite')</p> <code>'overwrite'</code> <code>options</code> <code>Optional[Dict[str, Any]]</code> <p>Additional options to pass to the underlying engine</p> <code>None</code> <p>Returns:</p> Type Description <p>None</p>"},{"location":"apis/secrets.read/","title":"secrets.read","text":"<p>               Bases: <code>ABC</code></p>"},{"location":"apis/secrets.read/#lakeops.core.secrets.SecretManager.read","title":"<code>read(key, scope=None, redacted=True)</code>  <code>abstractmethod</code>","text":"<p>Read a secret value for the given key and optional scope</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key name of the secret to read</p> required <code>scope</code> <code>Optional[str]</code> <p>Optional scope/namespace for the secret</p> <code>None</code> <code>redacted</code> <code>bool</code> <p>Whether to redact value instead of actual secret</p> <code>True</code> <p>Returns:</p> Name Type Description <code>string</code> <code>str</code> <p>The secret value or redacted version if redacted is True</p>"},{"location":"apis/secrets.write/","title":"secrets.write","text":"<p>               Bases: <code>ABC</code></p>"},{"location":"apis/secrets.write/#lakeops.core.secrets.SecretManager.write","title":"<code>write(key, value, scope=None)</code>  <code>abstractmethod</code>","text":"<p>Write a secret value for the given key and optional scope</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key name of the secret to write</p> required <code>value</code> <code>str</code> <p>The secret value to write</p> required <code>scope</code> <code>Optional[str]</code> <p>Optional scope/namespace for the secret</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p>"},{"location":"guide/engines/","title":"Engines","text":"<p>LakeOps supports multiple execution engines:</p> <pre><code># Default\npip install lakeops\n\n# Install pyspark engine\npip install lakeops[spark]\n\n# Install trino engine\npip install lakeops[trino]\n\n# Install Google Sheets engine\npip install lakeops[gsheet]\n\n# Install spark, trino, gsheet engines\npip install lakeops[spark,trino,gsheet]\n\n# NEW: Install spark connect engine\npip install lakeops[spark_connect]\n\n</code></pre>"},{"location":"guide/engines/#polars-engine","title":"Polars Engine","text":"<p>Default engine optimized for single-node operations, working with <code>polars.DataFrame</code>.</p> <pre><code>ops = LakeOps()  # Uses PolarsEngine by default\n</code></pre>"},{"location":"guide/engines/#spark-engine","title":"Spark Engine","text":"<p>For distributed processing and working with <code>pyspark.sql.DataFrame</code>:</p> <pre><code>from lakeops.core.engines import SparkEngine\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.getOrCreate()\nengine = SparkEngine(spark)\nops = LakeOps(engine)\n\n</code></pre>"},{"location":"guide/engines/#trino-engine","title":"Trino Engine","text":"<pre><code>from lakeops.core.engines import TrinoEngine, TrinoEngineConfig\n\ntrino_config = TrinoEngineConfig(\n    host=\"localhost\",\n    port=8080,\n    user=\"test_user\",\n    catalog=\"test_catalog\",\n    schema=\"test_schema\",\n    password=\"test_password\",\n)\nengine = TrinoEngine(trino_config)\nops = LakeOps(engine)\n\n</code></pre>"},{"location":"guide/engines/#google-sheets-engine","title":"Google Sheets Engine","text":"<pre><code>from lakeops.core.engines import GoogleSheetsEngine\n\ncredentials = {\n    \"type\": \"service_account\",\n    \"project_id\": \"api-project-XXX\",\n    \"private_key_id\": \"2cd \u2026 ba4\",\n    \"private_key\": \"-----BEGIN PRIVATE KEY-----\\nNrDyLw \u2026 jINQh/9\\n-----END PRIVATE KEY-----\\n\",\n    \"client_email\": \"473000000000-yoursisdifferent@developer.gserviceaccount.com\",\n    \"client_id\": \"473 \u2026 hd.apps.googleusercontent.com\",\n    ...\n}\n\nengine = GoogleSheetsEngine(credentials)\nops = LakeOps(engine)\n\n</code></pre>"},{"location":"guide/working_with_delta_lake/","title":"Working with Delta Lake","text":"<p>Delta Lake provides ACID transactions, scalable metadata handling, and unifies streaming and batch data processing. Here's how to use lakeops with Delta Lake format.</p>"},{"location":"guide/working_with_delta_lake/#setup","title":"Setup","text":"<pre><code>from pyspark.sql import SparkSession\n\nspark = (\n    SparkSession.builder.config(\n        \"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\"\n    )\n    .config(\n        \"spark.sql.catalog.spark_catalog\",\n        \"org.apache.spark.sql.delta.catalog.DeltaCatalog\",\n    )\n    .getOrCreate()\n)\n\nengine = SparkEngine(spark)\nops = LakeOps(engine)\n</code></pre>"},{"location":"guide/working_with_delta_lake/#reading-delta-tables","title":"Reading Delta Tables","text":"<pre><code>\n# Read from path\ndf = ops.read(\"s3://path/to/table\", format=\"delta\")\n\n# Read from table name\ndf = ops.read(\"table_name\", format=\"delta\")\n\n# Show table\ndf.show(truncate=False)\n\n</code></pre>"},{"location":"guide/working_with_delta_lake/#writing-delta-tables","title":"Writing Delta Tables","text":"<pre><code>\n# Write to path\nops.write(df, \"s3://path/to/table\", format=\"delta\")\n\n# Write to table name\nops.write(df, \"table_name\", format=\"delta\")\n\n</code></pre>"},{"location":"guide/working_with_google_sheets/","title":"Working with Google Sheets","text":"<p>LakeOps provides seamless integration with Google Sheets, allowing you to read and write data directly between your data lake and spreadsheets.</p>"},{"location":"guide/working_with_google_sheets/#quick-start","title":"Quick Start","text":"<pre><code>from lakeops.core.engines import GoogleSheetsEngine\nfrom lakeops.core.secrets import DatabricksSecretManager\n\nsecrets = DatabricksSecretManager()\ncredentials = secrets.read(\"google_credentials\", scope=\"production\")\nlake = GoogleSheetsEngine(credentials)\n</code></pre>"},{"location":"guide/working_with_google_sheets/#authentication","title":"Authentication","text":"<p>LakeOps uses Google Service Account for authentication with Google Sheets API. Here's how to set it up:</p> <ol> <li> <p>Create Service Account:</p> <ul> <li>Go to Google Cloud Console</li> <li>Create a new project or select existing one</li> <li>Enable Google Sheets API for your project</li> <li>Go to \"IAM &amp; Admin\" &gt; \"Service Accounts\"</li> <li>Click \"Create Service Account\"</li> <li>Download the JSON key file</li> </ul> </li> <li> <p>Store Service Account in LakeOps Secrets:</p> </li> </ol> <pre><code>secrets.write(\"google_credentials\", \"&lt;SECRETS FILE CONTENT&gt;\", scope=\"production\")\n</code></pre>"},{"location":"guide/working_with_google_sheets/#features","title":"Features","text":""},{"location":"guide/working_with_google_sheets/#reading-data","title":"Reading data","text":"<pre><code># Read from Google Sheet\ndf = lake.read(\n    \"1234567890abcdef\", &lt;-- Google Sheet ID\n    format=\"gsheet\",\n)\n</code></pre>"},{"location":"guide/working_with_google_sheets/#writing-data","title":"Writing data","text":"<pre><code>import polars as pl\n\n# Write to Google Sheet\nsales_data = pl.DataFrame({\n    \"date\": [\"2024-01-01\", \"2024-01-02\"],\n    \"revenue\": [1000, 1200]\n})\n\nlake.write(\n    sales_data,\n    \"1234567890abcdef\",  &lt;-- Google Sheet ID\n    format=\"gsheet\",\n)\n\n</code></pre>"},{"location":"guide/working_with_google_sheets/#best-practices","title":"Best Practices","text":"<ol> <li>Use service accounts for production workloads</li> <li>Use batch operations for large datasets</li> </ol>"},{"location":"guide/working_with_iceberg/","title":"Working with Iceberg","text":"<p>Apache Iceberg is a high-performance format for huge analytic tables that provides ACID transactions, schema evolution, and efficient querying. Here's how to use lakeops with Iceberg format.</p>"},{"location":"guide/working_with_iceberg/#setup","title":"Setup","text":"<pre><code>from pyspark.sql import SparkSession\n\nspark = (\n    SparkSession.builder.config(\n        \"spark.sql.extensions\",\n        \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n    )\n    .config(\n        \"spark.sql.catalog.spark_catalog\",\n        \"org.apache.iceberg.spark.SparkSessionCatalog\",\n    )\n    .config(\"spark.sql.catalog.spark_catalog.type\", \"hive\")\n    .config(\"spark.sql.catalog.local\", \"org.apache.iceberg.spark.SparkCatalog\")\n    .config(\"spark.sql.catalog.local.type\", \"hadoop\")\n    .config(\"spark.sql.catalog.local.warehouse\", \"/app/data\")\n    .config(\"spark.sql.defaultCatalog\", \"local\")\n    .getOrCreate()\n)\n\nengine = SparkEngine(spark)\nops = LakeOps(engine)\n</code></pre>"},{"location":"guide/working_with_iceberg/#reading-iceberg-tables","title":"Reading Iceberg Tables","text":"<pre><code># Read from path\ndf = ops.read(\"s3://path/to/table\", format=\"iceberg\")\n\n# Read from table name\ndf = ops.read(\"local.db.table_name\", format=\"iceberg\")\n# Show table\ndf.show(truncate=False)\n</code></pre>"},{"location":"guide/working_with_iceberg/#writing-iceberg-tables","title":"Writing Iceberg Tables","text":"<p>At this time, only writing to table name is supported</p> <pre><code># Write to table name\nops.write(df, \"local.db.table_name\", format=\"iceberg\")\n</code></pre>"},{"location":"guide/working_with_secrets/","title":"Working with Secrets","text":"<p>LakeOps provides a flexible secrets management system that supports multiple backend implementations.</p>"},{"location":"guide/working_with_secrets/#quick-start","title":"Quick Start","text":"<pre><code>from lakeops.core.secrets import SQLiteSecretManager\n# Using SQLite backend (local development)\nsecret_manager = SQLiteSecretManager()\nsecret_manager.write(\"api_key\", \"my-secret-value\")\nvalue = secret_manager.read(\"api_key\")  # Returns: ********value\n</code></pre>"},{"location":"guide/working_with_secrets/#available-backends","title":"Available Backends","text":""},{"location":"guide/working_with_secrets/#sqlite-backend","title":"SQLite Backend","text":"<p>Perfect for local development and testing. Stores secrets in a local SQLite database.</p> <pre><code>from lakeops.core.secrets import SQLiteSecretManager\nmanager = SQLiteSecretManager(\"secrets.db\")\nmanager.write(\"key\", \"value\")\nmanager.read(\"key\", show_redacted=True)  # Returns redacted value\nmanager.read(\"key\", show_redacted=False)  # Returns full value\n</code></pre>"},{"location":"guide/working_with_secrets/#databricks-backend","title":"Databricks Backend","text":"<p>Integration with Databricks Secrets API using the official databricks-sdk package. For details on authentication, refer to the official documentation.</p> <pre><code>from lakeops.core.secrets import DatabricksSecretManager\n\nmanager = DatabricksSecretManager()\nmanager.write(\"key\", \"value\", scope=\"my-scope\")\nmanager.read(\"key\", scope=\"my-scope\")\n</code></pre>"},{"location":"guide/working_with_secrets/#hashicorp-vault-backend","title":"Hashicorp Vault Backend","text":"<p>Integration with Hashicorp Vault KV v2 engine using the hvac library.</p>"},{"location":"guide/working_with_secrets/#token-authentication-default","title":"Token Authentication (Default)","text":"<pre><code>from lakeops.core.secrets import VaultSecretManager\n\nmanager = VaultSecretManager(\n    url=\"https://vault.example.com:8200\",\n    token=\"your-vault-token\"\n)\n</code></pre>"},{"location":"guide/working_with_secrets/#jwtoidc-authentication","title":"JWT/OIDC Authentication","text":"<pre><code>manager = VaultSecretManager(\n    url=\"https://vault.example.com:8200\",\n    auth_method=\"jwt\",\n    role=\"your-vault-role\",\n    jwt_token=\"your-jwt-token\"\n)\n</code></pre>"},{"location":"guide/working_with_secrets/#kubernetes-authentication","title":"Kubernetes Authentication","text":"<p>This method automatically reads the service account token from the default Kubernetes path.</p> <pre><code>manager = VaultSecretManager(\n    url=\"https://vault.example.com:8200\",\n    auth_method=\"kubernetes\",\n    role=\"your-k8s-vault-role\"\n)\n</code></pre>"},{"location":"guide/working_with_secrets/#operations","title":"Operations","text":"<pre><code># Write a secret (uses path: {scope}/{key} or just {key})\nmanager.write(\"api_key\", \"super-secret-value\", scope=\"my-project\")\n\n# Read a secret (returns redacted by default)\nvalue = manager.read(\"api_key\", scope=\"my-project\", redacted=True)\n# Returns: *************alue\n</code></pre>"},{"location":"guide/working_with_secrets/#best-practices","title":"Best Practices","text":"<ol> <li>Always use scopes in production environments</li> <li>Use redacted values when logging</li> <li>Rotate secrets periodically</li> <li>Use environment-specific scopes (dev, staging, prod)</li> </ol>"},{"location":"guide/working_with_secrets/#security-notes","title":"Security Notes","text":"<ul> <li>Secrets are automatically redacted when read (show last 4 characters)</li> <li>SQLite backend stores secrets locally - use only for development</li> <li>Databricks backend leverages enterprise-grade security</li> </ul>"}]}